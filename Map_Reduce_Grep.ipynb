{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will play around with \"map reduce\" frameworks using data scraped from Wikipedia. We'll implement a simplified version of the `grep` command-line utility to search for data in 54 megabytes worth of articles scraped from Wikipedia (saved in the 'wiki' folder)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create three processes in total. We will run each process twice, with a 'slow' version processing the entire group of files in one batch, and a 'fast' version using a map reduce approach. We will them compare the time differences between 'slow' and 'fast' versions (and ensure they are identical). We'll test all the processes using the target word 'data.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import libraries and define our map reduce function, appropriately named `map_reduce`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import functools\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def make_chunks(data, num_chunks):\n",
    "    chunk_size = math.ceil(len(data) / num_chunks)\n",
    "    return [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "\n",
    "def map_reduce(data, num_processes, mapper, reducer):\n",
    "    chunks = make_chunks(data, num_processes)\n",
    "    pool = Pool(num_processes)\n",
    "    chunk_results = pool.map(mapper, chunks)\n",
    "    return functools.reduce(reducer, chunk_results)\n",
    "\n",
    "def run_fns(f1, f2):\n",
    "    start = time.time()\n",
    "    slow = f1([os.path.join('wiki', file) for file in files])\n",
    "    end = time.time()\n",
    "    slow_time = end - start\n",
    "\n",
    "    start = time.time()\n",
    "    fast = f2()\n",
    "    end = time.time()\n",
    "    fast_time = end - start\n",
    "    \n",
    "    assert slow == fast\n",
    "    print(round(slow_time,2), round(fast_time,2), round(fast_time/slow_time,2))\n",
    "    return (slow, fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bay_of_ConcepciC3B3n.html',\n",
       " 'Bye_My_Boy.html',\n",
       " 'Valentin_Yanin.html',\n",
       " 'Kings_XI_Punjab_in_2014.html',\n",
       " 'William_Harvey_Lillard.html']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list the first five scraped files\n",
    "\n",
    "files = os.listdir('wiki')\n",
    "files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499797"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the total number of lines by looping over every file\n",
    "\n",
    "lines = 0\n",
    "for file in [os.path.join('wiki', file) for file in files]:\n",
    "    with open(file) as f:\n",
    "        lines += len(f.readlines())\n",
    "        \n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499797"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the total number of lines but this time by using the map_reduce function\n",
    "\n",
    "def map_files(file_list):\n",
    "    lines = 0\n",
    "    for file in [os.path.join('wiki', file) for file in file_list]:\n",
    "        with open(file) as f:\n",
    "            lines += len(f.readlines())\n",
    "    return lines\n",
    "\n",
    "map_reduce(files, 8, map_files, lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Grep` Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first process goes through each file and check to see if the 'target' word appears in the file's HTML. The process returns a dictionary with the file name as key and the lines that contain the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.51 0.32 0.63\n"
     ]
    }
   ],
   "source": [
    "def map_grep(file_list):\n",
    "    greps = {}\n",
    "    for i, file in enumerate(file_list):\n",
    "        with open(file) as f:\n",
    "            lines = [line for line in f.readlines()]\n",
    "        for idx, line in enumerate(lines):\n",
    "            if target in line:\n",
    "                if file in greps: greps[file].append(idx)\n",
    "                else: greps[file] = [idx]\n",
    "    return greps\n",
    "\n",
    "def reduce_grep(d1, d2):\n",
    "    d1.update(d2)\n",
    "    return d1\n",
    "\n",
    "def map_reduce_grep(num_processes=8):\n",
    "    file_names = [os.path.join('wiki', file) for file in files]\n",
    "    return map_reduce(file_names, num_processes, map_grep, reduce_grep)\n",
    "\n",
    "target = 'data'\n",
    "grep_slow, grep_fast = run_fns(map_grep, map_reduce_grep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Grep` Process with Case Insensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process goes through each file and check to see if the 'target' word appears in the file's HTML, but first converts the HTML to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72 0.44 0.62\n"
     ]
    }
   ],
   "source": [
    "def map_grep_insensitive(file_list):\n",
    "    greps = {}\n",
    "    for i, file in enumerate(file_list):\n",
    "        with open(file) as f:\n",
    "            lines = [line for line in f.readlines()]\n",
    "        for idx, line in enumerate(lines):\n",
    "            if target in line.lower():\n",
    "                if file in greps: greps[file].append(idx)\n",
    "                else: greps[file] = [idx]\n",
    "    return greps\n",
    "\n",
    "def map_reduce_grep_insensitive(num_processes=8):\n",
    "    file_names = [os.path.join('wiki', file) for file in files]\n",
    "    return map_reduce(file_names, num_processes, map_grep_insensitive, reduce_grep)\n",
    "\n",
    "target = 'data'\n",
    "grep_insensitive_slow, grep_insensitive_fast = run_fns(map_grep_insensitive, map_reduce_grep_insensitive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll print out the files for which the case insensitive process yielded more matches than the original process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1953E2809354_FA_Cup_qualifying_rounds: 1\n",
      "83_(number): 1\n",
      "A_Beautiful_Valley: 1\n",
      "Acceptance_(Heroes): 1\n",
      "Agaritine_gammaglutamyltransferase: 2\n",
      "Alex_Kurtzman: 1\n",
      "Amborella: 1\n",
      "Antibiotic_use_in_livestock: 1\n",
      "Appa_(film): 1\n",
      "Avengers_Academy: 1\n",
      "Bahmanabade_Olya: 1\n",
      "Battle_of_Wattignies: 1\n",
      "Benny_Lee: 1\n",
      "Bias: 1\n",
      "Bibiana_Beglau: 1\n",
      "Blue_SWAT: 1\n",
      "Boardman_Township_Mahoning_County_Ohio: 1\n",
      "Brownfield_(software_development): 1\n",
      "C11orf30: 1\n",
      "C389cole_des_Mines_de_Douai: 1\n",
      "Camp_Nelson_Confederate_Cemetery: 1\n",
      "Claire_Danes: 2\n",
      "Claudia_Neidig: 1\n",
      "Cobble_Hill_Brooklyn: 1\n",
      "Code_page_1023: 3\n",
      "Colchester_Village_Historic_District: 1\n",
      "Companys_procC3A9s_a_Catalunya: 1\n",
      "Copamyntis_infusella: 1\n",
      "Craig_Chester: 1\n",
      "Cryptographic_primitive: 1\n",
      "CurtissWright_Hangar_(Columbia_South_Carolina): 1\n",
      "Daniel_Cerone: 1\n",
      "Danish_Maritime_Safety_Administration: 1\n",
      "Dean_Kukan: 1\n",
      "Demographics_of_American_Samoa: 1\n",
      "Derek_Acorah: 1\n",
      "Devil_on_Horseback: 1\n",
      "Dewoitine_D.21: 1\n",
      "Don_Parsons_(ice_hockey): 1\n",
      "Don_Raye: 1\n",
      "Doumanaba: 1\n",
      "Dragnet_(franchise): 6\n",
      "Ek_Dil_Sau_Afsane: 1\n",
      "Embraer_Unidade_GaviC3A3o_Peixoto_Airport: 1\n",
      "Exploratorium_(film): 1\n",
      "Failing_Office_Building: 1\n",
      "Filip_Pyrochta: 1\n",
      "Foulonia: 1\n",
      "Frost_Township_Michigan: 1\n",
      "Functoid: 1\n",
      "Furto_di_sera_bel_colpo_si_spera: 1\n",
      "Golabkhvaran: 1\n",
      "Gordon_Bau: 2\n",
      "Gulliver_Mickey: 1\n",
      "HD_90156: 1\n",
      "Harry_Hill_Bandholtz: 1\n",
      "Hayateumi_Hidehito: 1\n",
      "Holly_Golightly_(comics): 1\n",
      "Imperial_Venus_(film): 1\n",
      "Ingrid_GuimarC3A3es: 1\n",
      "Jack_Goes_Home: 1\n",
      "Jazz_in_Turkey: 1\n",
      "Jon_Mullich: 1\n",
      "Jonathan_A._Goldstein: 1\n",
      "Jules_Verne_ATV: 2\n",
      "Julien_Boisselier: 1\n",
      "KMTZ: 1\n",
      "Kate_Harwood: 2\n",
      "Kattukukke: 1\n",
      "Kim_Yonghwa: 2\n",
      "King_Parker_House: 1\n",
      "Kokan_Colony: 1\n",
      "Kul_Gul: 1\n",
      "L._Fry: 1\n",
      "Lake_County_Examiner: 1\n",
      "Lis_LC3B8wert: 1\n",
      "List_of_Spaghetti_Western_films: 2\n",
      "List_of_Uzbek_films_of_2014: 1\n",
      "List_of_molecular_graphics_systems: 4\n",
      "List_of_people_from_Bangor_Maine: 7\n",
      "Lower_Blackburn_Grade_Bridge: 1\n",
      "Manhattan_Murder_Mystery: 1\n",
      "Maniitsoq_structure: 3\n",
      "Medicago_murex: 1\n",
      "Meleh_Kabude_Sofla: 1\n",
      "Mirisah: 1\n",
      "Morgana_King: 1\n",
      "Morning_Glory_(2010_film): 1\n",
      "Mudramothiram: 1\n",
      "Nuno_Leal_Maia: 1\n",
      "Old_Mill_Creek_Illinois: 1\n",
      "Oldfield_Baby_Great_Lakes: 1\n",
      "Ordinary_Virginia: 1\n",
      "PTPRS: 1\n",
      "Panchamrutham: 1\n",
      "Peter_Collingwood: 1\n",
      "Pictogram: 1\n",
      "Precorrin6A_reductase: 2\n",
      "Pushkar: 1\n",
      "Qalat_Kat: 1\n",
      "Rally_for_Democracy_and_Progress_(Benin): 1\n",
      "Roxbury_Presbyterian_Church: 1\n",
      "Rudy_The_Rudy_Giuliani_Story: 1\n",
      "Sahanpur: 1\n",
      "SalemAuburn_Streets_Historic_District: 1\n",
      "Saravan_Gilan: 1\n",
      "Shabbir_Kumar: 1\n",
      "Shoreyjehye_Do: 1\n",
      "Shpolskii_matrix: 2\n",
      "Smilax_laurifolia: 1\n",
      "Sol_Eclipse: 1\n",
      "Swathi_Chinukulu: 1\n",
      "Syngenor: 1\n",
      "Table_Point_Formation: 1\n",
      "Taipa_HousesE28093Museum: 2\n",
      "Taylor_Williamson: 1\n",
      "Teiji_Ito: 1\n",
      "The_Audacity_to_Podcast: 2\n",
      "The_Future_(film): 1\n",
      "The_Gentleman_Without_a_Residence_(1915_film): 1\n",
      "Tim_Spencer_(singer): 1\n",
      "Tomohiko_ItC58D_(director): 2\n",
      "Tropical_sprue: 1\n",
      "Typhoon_Hester_(1952): 1\n",
      "Urs_Burkart: 1\n",
      "Viva_Villa: 2\n",
      "Vojin_C486etkoviC487: 1\n",
      "WLSR: 1\n",
      "Weiser_River: 1\n",
      "West_Park_Bridge: 1\n",
      "Westchester_Los_Angeles: 1\n",
      "Wilhelm_Wagenfeld_House: 1\n",
      "Wilson_Global_Explorer: 1\n",
      "WintersWimberley_House: 1\n"
     ]
    }
   ],
   "source": [
    "for file in sorted(list(grep_insensitive_fast.keys())):\n",
    "    if grep_insensitive_fast[file] != grep_fast[file]:\n",
    "        print(f\"{file[5:-5]}: {len(grep_insensitive_fast[file]) - len(grep_fast[file])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Grep` Process with Match Indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final process does the same as the previous one, except it returns the line number per file, and the index of the line (possibly more than one) for each match. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.92 1.37 0.47\n"
     ]
    }
   ],
   "source": [
    "def map_grep_indices(file_list):\n",
    "    greps = {}\n",
    "    for i, file in enumerate(file_list):\n",
    "        with open(file) as f:\n",
    "            lines = [line for line in f.readlines()]\n",
    "        for idx, line in enumerate(lines):\n",
    "            matches = [m.start() for m in re.finditer(target, line.lower())]\n",
    "            if len(matches) > 0:\n",
    "                for m in matches:\n",
    "                    if file in greps: greps[file].append((idx, m))\n",
    "                    else: greps[file] = [(idx, m)]\n",
    "    return greps\n",
    "\n",
    "def map_reduce_grep_indices(num_processes=8):\n",
    "    file_names = [os.path.join('wiki', file) for file in files]\n",
    "    return map_reduce(file_names, num_processes, map_grep_indices, reduce_grep)\n",
    "\n",
    "target = 'data'\n",
    "grep_indices_slow, grep_indices_fast = run_fns(map_grep_indices, map_reduce_grep_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll convert the results to a `pandas` dataframe and sample 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>line</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20587</th>\n",
       "      <td>wiki/Cyclone_Gamede.html</td>\n",
       "      <td>36</td>\n",
       "      <td>608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10039</th>\n",
       "      <td>wiki/Bulbophyllum_biflorum.html</td>\n",
       "      <td>121</td>\n",
       "      <td>453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3927</th>\n",
       "      <td>wiki/Tim_Spencer_(singer).html</td>\n",
       "      <td>143</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19290</th>\n",
       "      <td>wiki/United_Nations_Security_Council_Resolutio...</td>\n",
       "      <td>392</td>\n",
       "      <td>1139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19277</th>\n",
       "      <td>wiki/United_Nations_Security_Council_Resolutio...</td>\n",
       "      <td>113</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8398</th>\n",
       "      <td>wiki/List_of_lakes_in_Independence_County_Arka...</td>\n",
       "      <td>107</td>\n",
       "      <td>787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10662</th>\n",
       "      <td>wiki/Kendalia_Texas.html</td>\n",
       "      <td>62</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9170</th>\n",
       "      <td>wiki/Asparuh_Peak.html</td>\n",
       "      <td>143</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14756</th>\n",
       "      <td>wiki/List_of_Polish_consorts.html</td>\n",
       "      <td>825</td>\n",
       "      <td>645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6499</th>\n",
       "      <td>wiki/List_of_Bellator_MMA_events.html</td>\n",
       "      <td>2071</td>\n",
       "      <td>1152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    file  line  index\n",
       "20587                           wiki/Cyclone_Gamede.html    36    608\n",
       "10039                    wiki/Bulbophyllum_biflorum.html   121    453\n",
       "3927                      wiki/Tim_Spencer_(singer).html   143    159\n",
       "19290  wiki/United_Nations_Security_Council_Resolutio...   392   1139\n",
       "19277  wiki/United_Nations_Security_Council_Resolutio...   113    409\n",
       "8398   wiki/List_of_lakes_in_Independence_County_Arka...   107    787\n",
       "10662                           wiki/Kendalia_Texas.html    62    593\n",
       "9170                              wiki/Asparuh_Peak.html   143    785\n",
       "14756                  wiki/List_of_Polish_consorts.html   825    645\n",
       "6499               wiki/List_of_Bellator_MMA_events.html  2071   1152"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "for file in grep_indices_fast:\n",
    "    for entry in grep_indices_fast[file]:\n",
    "        row = pd.DataFrame([{'file': file, 'line': entry[0], 'index' : entry[1]}])\n",
    "        df = pd.concat([df, row])\n",
    "\n",
    "df = df.reset_index()\n",
    "df.sample(10).iloc[:, -3:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
